{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09 - R-Squared, R, r, TSS, ESS, and RSS\n",
    "\n",
    "## R Squared\n",
    "\n",
    "* $SS_{res} = \\sum{(y - \\hat{y})^2}$\n",
    "* $SS_{tot} = \\sum{(y - \\bar{y})^2}$\n",
    "* $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "In statistics, the Pearson correlation coefficient ― also known as Pearson's r ― is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationship or correlation. As a simple example, one would expect the age and height of a sample of teenagers from a high school to have a Pearson correlation coefficient significantly greater than 0, but less than 1 (as 1 would represent an unrealistically perfect correlation).\n",
    "\n",
    "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "* r shows correlation between x and y\n",
    "* r squared shows strength of model, the proportion of the variance y that can be explained by X in a linear regression model\n",
    "\n",
    "## R Squared, R, and r\n",
    "\n",
    "* R-Squared: Coefficient of Determination, proportion of the variation in the dependent variable that is predictable from the independent variable(s)\n",
    "* R: Coefficient of multiple correlation, a measure of how well a given variable can be predicted using a linear function of a set of other variables\n",
    "* r: Pearson's r, cCorrelation is different from regression, as it does not assume any sort of dependency between two quantitative variables and it is only meant to express their joint variability\n",
    "* https://www.r-bloggers.com/2022/11/the-coefficient-of-determination-is-it-the-r-squared-or-r-squared/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rs with Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r with df.corr()            Feature 1         y\n",
      "Feature 1   1.000000  0.987579\n",
      "y           0.987579  1.000000\n",
      "r with scipy 0.9875791397513904\n",
      "r^2 0.9753125572720964\n",
      "R squared (model2 score):  0.975312557272096\n",
      "R squared (model score with X_train and y_train):  0.97584774237287\n",
      "R squared (metric from sklearn of y_test and y_hat) 0.9735012189411446\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# create dataframe\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=13)\n",
    "df = pd.DataFrame(data=X, columns=['Feature 1'])\n",
    "df['y'] = y\n",
    "\n",
    "# train test split; change the random_state to 42 if not using the generated dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df['y'], test_size=.25, random_state=42)\n",
    "\n",
    "# create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# create and train the model\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X, y)\n",
    "\n",
    "# test set prediction results\n",
    "y_hat = model.predict(X_test)\n",
    "print('r with df.corr()', df.corr())\n",
    "pcorr, _ = pearsonr(X.flatten(), y)\n",
    "print('r with scipy', pcorr)\n",
    "print('r^2', pcorr**2)\n",
    "print('R squared (model2 score): ', model2.score(X, y))\n",
    "print('R squared (model score with X_train and y_train): ', model.score(X_train, y_train))\n",
    "print('R squared (metric from sklearn of y_test and y_hat)', r2_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rs with Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared (model score):  0.9657387189006659\n",
      "R squared (metric from sklearn of y_test and y_hat) 0.9592641536685931\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# create dataframe\n",
    "X, y = make_regression(n_samples=1000, n_features=3, noise=13)\n",
    "df = pd.DataFrame(data=X, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "df['y'] = y\n",
    "\n",
    "# train test split; change the random_state to 42 if not using the generated dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df['y'], test_size=.25, random_state=42)\n",
    "\n",
    "# create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test set prediction results\n",
    "y_hat = model.predict(X_test)\n",
    "print('R squared (model score): ', model.score(X_train, y_train))\n",
    "print('R squared (metric from sklearn of y_test and y_hat)', r2_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Sum of Squares\n",
    "\n",
    "According to Wikipedia, if $\\bar{y}$ is the mean of the observed data:\n",
    "\n",
    "$\n",
    "\\bar{y} = \\frac{1}{N}\\sum(y)\n",
    "$\n",
    "\n",
    "then the variability of the data set can be measured with two sums of squares formulas:\n",
    "\n",
    "The total sum of squares (proportional to the variance of the data):\n",
    "\n",
    "$\n",
    "SS_{tot} = \\sum(y - \\bar{y})^2\n",
    "$\n",
    "\n",
    "## Residual Sum of Squares\n",
    "\n",
    "The sum of squares of residuals, also called the residual sum of squares:\n",
    "\n",
    "$\n",
    "SS_{res} = \\sum(y - \\hat{y})^2\n",
    "$\n",
    "\n",
    "$\\hat{y}$ represents our predicted y.\n",
    "\n",
    "With this information, we can get r-squared:\n",
    "\n",
    "$\n",
    "R^2 = 1 - \\large{\\frac{SS_{res}}{SS_{tot}}}\n",
    "$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Residual_sum_of_squares\n",
    "\n",
    "## Explained Sum of Squares\n",
    "\n",
    "Another sum of squares not mentioned is Explained Sum of Squares:\n",
    "\n",
    "$\n",
    "ESS = \\sum(\\hat{y} - \\bar{y})^2\n",
    "$\n",
    "\n",
    "or\n",
    "\n",
    "$\n",
    "TSS = RSS + ESS\n",
    "$\n",
    "\n",
    "or \n",
    "\n",
    "$\n",
    "ESS = TSS - RSS\n",
    "$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Explained_sum_of_squares\n",
    "\n",
    "The formula for Adjusted R-Squared is:\n",
    "\n",
    "$\n",
    "R^2_{adj} = 1 - (1 - R^2)\\large{\\frac{n-1}{n - p - 1}}\n",
    "$\n",
    "\n",
    "or\n",
    "\n",
    "$\n",
    "R^2_{adj} = 1 - \\large{\\frac{\\frac{SS_{res}}{df_e}}{\\frac{SS_{tot}}{df_t}}}\n",
    "$\n",
    "\n",
    "where $df_t$ is the degrees of freedom n – 1 of the estimate of the population variance of the dependent variable, and $df_e$ is the degrees of freedom n – p – 1 of the estimate of the underlying population error variance. Note: p = parameters or features; n = observations\n",
    "\n",
    "The adjusted R2 can be negative, and its value will always be less than or equal to that of R2. Unlike R2, the adjusted R2 increases only when the increase in R2 (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance. If a set of explanatory variables with a predetermined hierarchy of importance are introduced into a regression one at a time, with the adjusted R2 computed each time, the level at which adjusted R2 reaches a maximum, and decreases afterward, would be the regression with the ideal combination of having the best fit without excess/unnecessary terms.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
